{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking an unknown number of objects\n",
    "\n",
    "While SVI can be used to learn components and assignments of a mixture model, pyro.contrib.tracking provides more efficient inference algorithms to estimate assignments. This notebook demonstrates how to use the `MarginalAssignmentPersistent` with EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.tracking.assignment import MarginalAssignmentPersistent\n",
    "from pyro.contrib.tracking.hashing import LSH\n",
    "from pyro.ops.newton import newton_step_2d\n",
    "from pyro.infer import SVI, TraceEnum_ELBO\n",
    "from pyro.optim import ClippedAdam, ASGD, SGD\n",
    "from pyro.util import warn_if_nan\n",
    "\n",
    "%matplotlib notebook\n",
    "pyro.enable_validation(True)\n",
    "smoke_test = ('CI' in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a model with deterministic dynamics, say sinusoids with known period but unknown phase and amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamics(num_frames):\n",
    "    time = torch.arange(num_frames,dtype=torch.float)*2*math.pi/num_frames\n",
    "    return torch.stack([time.cos(), time.sin()], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's tricky to define a fully generative model, so instead we'll separate our data generation process `generate_data()` from a factor graph `model()` that will be used in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(args):\n",
    "    # Object model.\n",
    "    num_objects = int(round(args.expected_num_objects))  # Deterministic.\n",
    "    states = dist.Normal(0., 1.).sample((num_objects, 2))\n",
    "\n",
    "    # Detection model.\n",
    "    emitted = dist.Bernoulli(args.emission_prob).sample((args.num_frames, num_objects))\n",
    "    num_spurious = dist.Poisson(args.expected_num_spurious).sample((args.num_frames,))\n",
    "    max_num_detections = int((num_spurious + emitted.sum(-1)).max())\n",
    "    observations = torch.zeros(args.num_frames, max_num_detections, 1+1) # position+confidence\n",
    "    positions = get_dynamics(args.num_frames).mm(states.t())\n",
    "    noisy_positions = dist.Normal(positions, args.emission_noise_scale).sample()\n",
    "    for t in range(args.num_frames):\n",
    "        j = 0\n",
    "        for i, e in enumerate(emitted[t]):\n",
    "            if e:\n",
    "                observations[t, j, 0] = noisy_positions[t, i]\n",
    "                observations[t, j, 1] = 1\n",
    "                j += 1\n",
    "        n = int(num_spurious[t])\n",
    "        if n:\n",
    "            observations[t, j:j+n, 0] = dist.Normal(0., 1.).sample((n,))\n",
    "            observations[t, j:j+n, 1] = 1\n",
    "\n",
    "    return states, positions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@poutine.broadcast\n",
    "def model(args, observations):\n",
    "    with pyro.iarange(\"objects\", args.max_num_objects):\n",
    "        exists = pyro.sample(\"exists\",\n",
    "                             dist.Bernoulli(args.expected_num_objects / args.max_num_objects))\n",
    "        with poutine.scale(scale=exists):\n",
    "            states = pyro.sample(\"states\", dist.Normal(0., 1.).expand([2]).independent(1))\n",
    "            positions = get_dynamics(args.num_frames).mm(states.t())\n",
    "    with pyro.iarange(\"detections\", observations.shape[1]):\n",
    "        with pyro.iarange(\"time\", args.num_frames):\n",
    "            # The combinatorial part of the log prob is approximated to allow independence.\n",
    "            is_observed = (observations[..., -1] > 0)\n",
    "            with poutine.scale(scale=is_observed.float()):\n",
    "                assign = pyro.sample(\"assign\",\n",
    "                                     dist.Categorical(torch.ones(args.max_num_objects + 1)))\n",
    "            is_spurious = (assign == args.max_num_objects)\n",
    "            is_real = is_observed & ~is_spurious\n",
    "            num_observed = is_observed.float().sum(-1, True)\n",
    "            # TODO Make these Bernoulli probs more plausible.\n",
    "            pyro.sample(\"is_real\",\n",
    "                        dist.Bernoulli(args.expected_num_objects / observations.shape[1]),\n",
    "                        obs=is_real.float())\n",
    "            pyro.sample(\"is_spurious\",\n",
    "                        dist.Bernoulli(args.expected_num_spurious / observations.shape[1]),\n",
    "                        obs=is_spurious.float())\n",
    "\n",
    "            # The remaining continuous part is exact.\n",
    "            observed_positions = observations[..., 0]\n",
    "            with poutine.scale(scale=is_real.float()):\n",
    "                bogus_position = positions.new_zeros(args.num_frames, 1)\n",
    "                augmented_positions = torch.cat([positions, bogus_position], -1)\n",
    "                predicted_positions = augmented_positions[:, assign]\n",
    "                pyro.sample(\"real_observations\",\n",
    "                            dist.Normal(predicted_positions, args.emission_noise_scale),\n",
    "                            obs=observed_positions)\n",
    "            with poutine.scale(scale=is_spurious.float()):\n",
    "                pyro.sample(\"spurious_observations\", dist.Normal(0., 1.),\n",
    "                            obs=observed_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_assign_logits(positions, observations, args):\n",
    "    log_likelihood = detection_log_likelihood(positions, observations, args)\n",
    "    assign_logits = log_likelihood[...,:-1] - log_likelihood[...,-1:]\n",
    "    assign_logits[log_likelihood[..., 0]== -float('inf')] = -float('inf')\n",
    "    return assign_logits\n",
    "\n",
    "def detection_log_likelihood(positions, observations, args):\n",
    "    real_dist = dist.Normal(positions.unsqueeze(-2), args.emission_noise_scale)\n",
    "    spurious_dist = dist.Normal(0., 1.)\n",
    "    is_observed = (observations[..., -1] > 0)\n",
    "    observed_positions = observations[..., 0].unsqueeze(-1)\n",
    "    a=(real_dist.log_prob(observed_positions) +\n",
    "                 math.log(args.expected_num_objects * args.emission_prob))\n",
    "    b= (spurious_dist.log_prob(observed_positions) +\n",
    "                                 math.log(args.expected_num_spurious))\n",
    "    assign_logits = torch.cat((a,b), dim=-1)\n",
    "    assign_logits[~is_observed] = -float('inf')\n",
    "    return assign_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "This implementation of merging function averages `states_loc`, and sums `exist_dist_probs` and `assign_dist_probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "def merge_objects(assignment, states_loc, threshold, exists_mask=None):\n",
    "    exist_dist_probs = assignment.exists_dist.probs\n",
    "    spurious_prob = assignment.assign_dist.probs[...,-1:]\n",
    "    assign_dist_probs = assignment.assign_dist.probs[...,:-1].permute(2,0,1)\n",
    "    distance2 = (states_loc.unsqueeze(-2) -\n",
    "                  states_loc.unsqueeze(-3)).pow(2).sum(-1)\n",
    "    if exists_mask is None:   \n",
    "        p_exist_threshold = 1e-5\n",
    "        exists_mask = assignment.exists_dist.probs > p_exist_threshold\n",
    "    ind = ((distance2 < threshold**2) &\n",
    "           exists_mask.unsqueeze(0) &\n",
    "           exists_mask.unsqueeze(-1)\n",
    "           ).nonzero()\n",
    "    if len(ind) == 0:\n",
    "        return assignment, states_loc\n",
    "    ind = ind[ind[:, 0] < ind[:, 1]]\n",
    "    states_loc = states_loc[exists_mask]\n",
    "    # setup data structures to cheaply search for nearest pairs\n",
    "    priority_queue = []\n",
    "    for i, j in ind:\n",
    "        heapq.heappush(priority_queue, (distance2[i, j], i.item(), j.item()))\n",
    "    lsh = LSH(1)\n",
    "    for i, state in enumerate(states_loc):\n",
    "        lsh.add(i, state)\n",
    "    next_id = len(states_loc)\n",
    "    states_loc = dict(enumerate(states_loc))\n",
    "    exist_dist_probs = dict(enumerate(exist_dist_probs))\n",
    "    assign_dist_probs = dict(enumerate(assign_dist_probs))\n",
    "    while priority_queue:\n",
    "        d1, i, j = heapq.heappop(priority_queue)\n",
    "        if i not in states_loc or j not in states_loc:\n",
    "            continue\n",
    "        k = next_id\n",
    "        next_id += 1\n",
    "        # Merging\n",
    "        states_loc[k] = (states_loc.pop(i) + states_loc.pop(j)) / 2\n",
    "        #merge_fn = lambda a,b: (a.pow(2) + b.pow(2)) / (a + b) #contra-mean\n",
    "        merge_fn = lambda a,b: (a.pow(1/2) + b.pow(1/2)) / (a + b) #inverse contra-mean\n",
    "        #merge_fn = lambda a,b: 2 / (1 / a + 1 / b) # harmonic mean\n",
    "        #merge_fn = lambda a,b: a + b # sum\n",
    "        exist_dist_probs[k] = merge_fn(exist_dist_probs.pop(i),exist_dist_probs.pop(j))\n",
    "        assign_dist_probs[k] = (assign_dist_probs.pop(i) + assign_dist_probs.pop(j))\n",
    "        exist_dist_probs[k].clamp_(min=0.0, max=1.0)\n",
    "        assign_dist_probs[k].clamp_(min=0.0, max=1.0)\n",
    "        lsh.remove(i)\n",
    "        lsh.remove(j)\n",
    "        lsh.add(k, states_loc[k])\n",
    "        for i in lsh.nearby(k):\n",
    "            if i == k:\n",
    "                continue\n",
    "            d2 = (states_loc[i] - states_loc[k]).pow(2).sum()\n",
    "            if d2 < threshold:\n",
    "                heapq.heappush(priority_queue, (d2, i, k))\n",
    "\n",
    "    ids = sorted(states_loc.keys())\n",
    "    states_loc = torch.stack([states_loc[i] for i in ids])\n",
    "    states_loc = torch.cat([states_loc, \n",
    "                            torch.zeros((assignment.num_objects-states_loc.shape[0],\n",
    "                                         states_loc.shape[1]),\n",
    "                                         dtype=states_loc.dtype,\n",
    "                                         layout=states_loc.layout,\n",
    "                                         device=states_loc.device)\n",
    "                               ])\n",
    "    \n",
    "    exist_dist_probs = torch.stack([exist_dist_probs[i] for i in ids])\n",
    "    assign_dist_probs = torch.stack([assign_dist_probs[i] for i in ids]).permute(1,2,0)\n",
    "    exists_mask = torch.arange(assignment.num_objects) < exist_dist_probs.shape[0]\n",
    "    assert exists_mask.sum() < 100\n",
    "    if assignment.num_objects > exist_dist_probs.shape[0]:\n",
    "        exist_dist_probs = torch.cat([exist_dist_probs, \n",
    "                                   torch.zeros((assignment.num_objects-exist_dist_probs.shape[0]),\n",
    "                                                 dtype=exist_dist_probs.dtype,\n",
    "                                                 layout=exist_dist_probs.layout,\n",
    "                                                 device=exist_dist_probs.device)\n",
    "                                   ],dim=-1)\n",
    "        assign_dist_probs = torch.cat([assign_dist_probs, \n",
    "                                   torch.zeros((assign_dist_probs.shape[0],\n",
    "                                                 assign_dist_probs.shape[1],\n",
    "                                                 assignment.num_objects-assign_dist_probs.shape[2]),\n",
    "                                                 dtype=assign_dist_probs.dtype,\n",
    "                                                 layout=assign_dist_probs.layout,\n",
    "                                                 device=assign_dist_probs.device),\n",
    "                                      ],dim=-1)\n",
    "    assign_dist_probs = torch.cat([assign_dist_probs,spurious_prob],dim=-1)\n",
    "    assignment.exists_dist = dist.Bernoulli(probs=exist_dist_probs)\n",
    "    assignment.assign_dist = dist.Categorical(probs=assign_dist_probs)\n",
    "    \n",
    "    return assignment, states_loc, exists_mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide\n",
    "This guide uses EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@poutine.broadcast\n",
    "def guide(args, observations):\n",
    "    # Initialize states randomly from the prior.\n",
    "    states_loc = pyro.param(\"states_loc\", lambda: torch.randn(args.max_num_objects, 2))\n",
    "    is_observed = (observations[..., -1] > 0) \n",
    "    exists_mask=torch.ones(args.max_num_objects,dtype=torch.uint8)\n",
    "    for em_iter in range(args.em_iters):\n",
    "        states_loc = states_loc.detach()\n",
    "        states_loc.requires_grad = True\n",
    "        positions = get_dynamics(args.num_frames).mm(states_loc.t())\n",
    "        \n",
    "        # E-step: compute soft assignments\n",
    "        with torch.no_grad():\n",
    "            assign_logits = compute_assign_logits(positions, observations, args)\n",
    "            exists_logits = torch.empty(args.max_num_objects).fill_(\n",
    "                math.log(args.expected_num_objects / args.max_num_objects))\n",
    "            exists_logits[~exists_mask]=-float('inf')\n",
    "            assignment = MarginalAssignmentPersistent(exists_logits, assign_logits,\n",
    "                                              args.bp_iters, bp_momentum=args.bp_momentum)\n",
    "            p_exists = assignment.exists_dist.probs\n",
    "            p_assign = assignment.assign_dist.probs\n",
    "        \n",
    "        log_likelihood = detection_log_likelihood(positions,observations, args)\n",
    "        loss = -(log_likelihood * p_assign).sum()\n",
    "        states_loc, states_cov = newton_step_2d(loss, states_loc,\n",
    "                                                args.emission_noise_scale # TODO: Play with this\n",
    "                                                )  # M-step\n",
    "        if args.merge_threshold >= 0.0:\n",
    "            assignment, states_loc, exists_mask = merge_objects(assignment, states_loc,\n",
    "                                                                args.merge_threshold, exists_mask)\n",
    "            #exists_logits = assignment.exists_dist.logits\n",
    "        warn_if_nan(states_loc, 'states_loc')\n",
    "        warn_if_nan(states_cov, 'states_cov')\n",
    "    \n",
    "    with pyro.iarange(\"objects\", args.max_num_objects):\n",
    "        exists = pyro.sample(\"exists\", assignment.exists_dist, infer={\"enumerate\": \"parallel\"})\n",
    "        with poutine.scale(scale=exists):\n",
    "            pyro.sample(\"states\", dist.Delta(states_loc).independent(1))\n",
    "    with pyro.iarange(\"detections\", observations.shape[1]):\n",
    "        with poutine.scale(scale=is_observed.float()):\n",
    "            with pyro.iarange(\"time\", args.num_frames):\n",
    "                pyro.sample(\"assign\", assignment.assign_dist, infer={\"enumerate\": \"parallel\"})\n",
    "\n",
    "    return assignment, states_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these plotting helpers before and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(message='', assignment=None, states_loc=None):\n",
    "    if (assignment is None) and (states_loc is None):\n",
    "        assignment, states_loc = guide(args, observations)\n",
    "    positions = get_dynamics(args.num_frames).mm(states_loc.t())\n",
    "    fig = pyplot.figure(figsize=(12,6))\n",
    "    fig.patch.set_color('white')\n",
    "    pyplot.plot(true_positions.numpy(), 'k--')\n",
    "    is_observed = (observations[..., -1] > 0)\n",
    "    pos = observations[..., 0]\n",
    "    time = torch.arange(args.num_frames).unsqueeze(-1).expand_as(pos)\n",
    "    pyplot.scatter(time[is_observed].view(-1).numpy(),\n",
    "                   pos[is_observed].view(-1).numpy(), color='k', marker='+',\n",
    "                   label='observation')\n",
    "    for i in range(args.max_num_objects):\n",
    "        p_exists = assignment.exists_dist.probs[i].item()\n",
    "        position = positions[:, i].detach().numpy()\n",
    "        pyplot.plot(position, alpha=p_exists, color='C0')\n",
    "    if args.expected_num_objects == 1:\n",
    "        p_exists = assignment.exists_dist.probs\n",
    "        mean = (p_exists * positions).sum(-1) / p_exists.sum(-1)\n",
    "        pyplot.plot(mean.detach().numpy(), 'r--', alpha=0.5, label='mean')\n",
    "    pyplot.title('Truth, observations, and {:0.1f} predicted tracks {}'.format(\n",
    "                 assignment.exists_dist.probs.sum().item(), message))\n",
    "    pyplot.plot([], 'k--', label='truth')\n",
    "    pyplot.plot([], color='C0', label='prediction')\n",
    "    pyplot.legend(loc='best')\n",
    "    pyplot.xlabel('time step')\n",
    "    pyplot.ylabel('position')\n",
    "    pyplot.tight_layout()\n",
    "\n",
    "def plot_exists_histogram(p_exists=None):\n",
    "    if p_exists is None:\n",
    "        p_exists = guide(args, observations)[0].exists_dist.probs\n",
    "    p_exists = p_exists.detach().numpy()\n",
    "    pyplot.figure(figsize=(6,4)).patch.set_color('white')\n",
    "    pyplot.plot(sorted(p_exists))\n",
    "    pyplot.ylim(0, None)\n",
    "    pyplot.xlim(0, len(p_exists))\n",
    "    pyplot.ylabel('p_exists')\n",
    "    pyplot.xlabel('rank')\n",
    "    pyplot.title('Prob(exists) of {} potential objects, total = {:0.2f}'.format(\n",
    "        len(p_exists), p_exists.sum()))\n",
    "    pyplot.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a global config object to make it easy to port code to `argparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('Args', (object,), {})  # A fake ArgumentParser.parse_args() result.\n",
    "\n",
    "args.num_frames = 40\n",
    "args.max_num_objects = 400\n",
    "args.expected_num_objects = 2.\n",
    "args.expected_num_spurious = 0.1  # If this is too small, BP will be unstable.\n",
    "args.emission_prob = 0.9          # If this is too large, BP will be unstable.\n",
    "args.emission_noise_scale = 0.1   # If this is too small, SVI will see flat gradients.\n",
    "args.bp_iters = 50\n",
    "args.bp_momentum =0.5\n",
    "args.svi_iters = 201\n",
    "args.em_iters = 10\n",
    "args.merge_threshold = 0.2\n",
    "assert args.max_num_objects >= args.expected_num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)\n",
    "true_states, true_positions, observations = generate_data(args)\n",
    "true_num_objects = len(true_states)\n",
    "max_num_detections = observations.shape[1]\n",
    "assert true_states.shape == (true_num_objects, 2)\n",
    "assert true_positions.shape == (args.num_frames, true_num_objects)\n",
    "assert observations.shape == (args.num_frames, max_num_detections, 1+1)\n",
    "print(\"generated {:d} detections from {:d} objects\".format(\n",
    "    (observations[..., -1] > 0).long().sum(), true_num_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "10 iterations of EM with and without merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)  # Use a different seed from data generation\n",
    "old_merge_threshold= args.merge_threshold\n",
    "args.merge_threshold = -1\n",
    "pyro.clear_param_store()\n",
    "plot_solution('after 10 EM (without merge)')\n",
    "plot_exists_histogram()\n",
    "args.merge_threshold = old_merge_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)  # Use a different seed from data generation\n",
    "pyro.clear_param_store()\n",
    "plot_solution('after 10 EM (with merge)')\n",
    "plot_exists_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
